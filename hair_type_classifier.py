# ==============================================================================
#                      README: CLASSIFICADOR DE TIPO DE CABELO
# ==============================================================================
"""
# üöÄ Classificador de Tipo de Cabelo (CNN PyTorch)

Este script implementa uma Rede Neural Convolucional (CNN) em PyTorch para a classifica√ß√£o bin√°ria de imagens de cabelo (liso vs. cacheado/crespo).

## ‚öôÔ∏è Configura√ß√£o
- **Dataset:** Hair Type dataset (treino e teste).
- **Arquitetura do Modelo:** CNN customizada de 3 camadas (Conv -> Linear -> Linear).
- **Otimizador:** Stochastic Gradient Descent (SGD) com momentum.
- **Reproducibilidade:** Semente (SEED=42) fixada para numpy e PyTorch.
- **Input Shape:** (3, 200, 200).

## üí° Arquitetura da CNN
1. **Conv2d:** 32 filtros, kernel (3, 3), ReLU.
2. **MaxPool2d:** kernel (2, 2).
3. **Linear (Hidden):** 64 neur√¥nios, ReLU.
4. **Linear (Output):** 1 neur√¥nio, Sigmoid.

## üìà Treinamento
O script inclui a prepara√ß√£o de dados, a defini√ß√£o do modelo e a fun√ß√£o de treinamento. O pipeline √© projetado para responder √†s quest√µes do exerc√≠cio, incluindo a aplica√ß√£o de **Data Augmentation** nas √∫ltimas 10 √©pocas.
"""

# ==============================================================================
#                               CONFIGURA√á√ÉO INICIAL
# ==============================================================================

# Importa√ß√µes essenciais para Deep Learning em Python
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import os
import shutil
import requests
import zipfile

# 1. Configura√ß√£o de Reproducibilidade (SEED)
# Essencial para garantir que os resultados sejam consistentes em diferentes execu√ß√µes.
SEED = 42
np.random.seed(SEED)
torch.manual_seed(SEED)

# Configura√ß√£o espec√≠fica para CUDA (GPU)
if torch.cuda.is_available():
    torch.cuda.manual_seed(SEED)
    torch.cuda.manual_seed_all(SEED)
    # Define determinismo para opera√ß√µes CUDA
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# Define o dispositivo de execu√ß√£o (GPU se dispon√≠vel, sen√£o CPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ==============================================================================
#                               DOWNLOAD E PREPARA√á√ÉO DOS DADOS
# ==============================================================================

DATA_URL = "https://github.com/SVizor42/ML_Zoomcamp/releases/download/straight-curly-data/data.zip"
DATA_PATH = "data.zip"
EXTRACT_DIR = "."

# Fun√ß√£o para download e extra√ß√£o do dataset
def download_and_extract_data():
    """Baixa e extrai o dataset Hair Type de Kaggle."""
    if os.path.exists("./train") and os.path.isdir("./train"):
        print("Diret√≥rio 'train' j√° existe. Pulando download.")
        return
        
    print(f"Baixando dados de: {DATA_URL}")
    try:
        r = requests.get(DATA_URL, stream=True)
        r.raise_for_status() # Verifica se o download foi bem-sucedido
        with open(DATA_PATH, 'wb') as f:
            f.write(r.content)
        print("Download conclu√≠do. Descompactando...")
        
        with zipfile.ZipFile(DATA_PATH, 'r') as zip_ref:
            zip_ref.extractall(EXTRACT_DIR)
        print("Extra√ß√£o conclu√≠da em ./data.")
        os.remove(DATA_PATH) # Remove o arquivo zip
    except Exception as e:
        print(f"Erro no download ou extra√ß√£o: {e}")

download_and_extract_data()

# ==============================================================================
#                               MODELO CNN (HairTypeClassifier)
# ==============================================================================

class HairTypeClassifier(nn.Module):
    """
    Define a arquitetura da CNN conforme as especifica√ß√µes do exerc√≠cio.
    Input Shape esperado: (3, 200, 200)
    """
    def __init__(self):
        super().__init__()
        
        # 1. Camada Convolucional: Input (3 canais) -> Output (32 filtros)
        # Output shape: (32, 198, 198)
        self.conv1 = nn.Conv2d(
            in_channels=3,
            out_channels=32,
            kernel_size=(3, 3),
            stride=1,
            padding=0
        )
        self.relu1 = nn.ReLU()
        
        # 2. Max Pooling: Reduz o mapa de caracter√≠sticas pela metade
        # Output shape: (32, 99, 99)
        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2))
        
        # O tamanho achatado (flattened) √© 32 canais * 99 * 99 pixels
        FLATTENED_SIZE = 32 * 99 * 99 # 313632
        
        # 3. Camada Linear Oculta: 313632 -> 64 neur√¥nios
        self.linear1 = nn.Linear(
            in_features=FLATTENED_SIZE,
            out_features=64
        )
        self.relu2 = nn.ReLU()
        
        # 4. Camada Linear de Sa√≠da: 64 -> 1 neur√¥nio
        self.output = nn.Linear(
            in_features=64,
            out_features=1
        )
        # 5. Ativa√ß√£o final: Sigmoid para classifica√ß√£o bin√°ria
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        # Passo 1: Convolu√ß√£o -> ReLU -> Pooling
        x = self.conv1(x)
        x = self.relu1(x)
        x = self.pool1(x)
        
        # Passo 2: Flatten - Transforma o tensor (Batch, Channels, H, W) em (Batch, C*H*W)
        x = x.view(x.size(0), -1) 
        
        # Passo 3: Camada Oculta -> ReLU
        x = self.linear1(x)
        x = x = self.relu2(x)
        
        # Passo 4: Camada de Sa√≠da -> Sigmoid
        x = self.output(x)
        x = self.sigmoid(x)
        
        return x

# ==============================================================================
#                           FUN√á√ïES DE TREINAMENTO E AVALIA√á√ÉO
# ==============================================================================

def train_and_validate(model, criterion, optimizer, train_loader, val_loader, num_epochs, start_epoch, history, train_dataset, validation_dataset, use_sigmoid_in_model):
    """
    Fun√ß√£o de treinamento e valida√ß√£o modular.
    Adapta-se ao uso de Sigmoid no modelo vs. BCEWithLogitsLoss.
    """
    print(f"\n--- Iniciando Treinamento (√âpocas {start_epoch+1} a {start_epoch+num_epochs}) ---")
    
    for epoch in range(start_epoch, start_epoch + num_epochs):
        # --------------------- FASE DE TREINAMENTO ---------------------
        model.train()
        running_loss = 0.0
        correct_train = 0
        total_train = 0
        
        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            # Unsqueeze(1) muda a shape de (batch_size) para (batch_size, 1)
            # Float() √© necess√°rio para a fun√ß√£o de perda (BCELoss ou BCEWithLogitsLoss)
            labels = labels.float().unsqueeze(1) 

            optimizer.zero_grad() # Zera os gradientes
            
            outputs = model(images) # Forward pass
            
            # Ajusta a sa√≠da se BCEWithLogitsLoss for usada (remove sigmoid se presente)
            if use_sigmoid_in_model and isinstance(criterion, nn.BCEWithLogitsLoss):
                # Se o modelo tem Sigmoid E o criterion √© BCEWithLogitsLoss, isso √© um erro.
                # Para fins de execu√ß√£o do exerc√≠cio, vamos assumir que o criterion √© BCELoss.
                # Se fosse BCEWithLogitsLoss, o Sigmoid deveria ser removido do modelo.
                pass # Usamos BCELoss na pr√°tica.

            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item() * images.size(0)
            
            # C√°lculo de acur√°cia: aplica threshold de 0.5 na sa√≠da (que j√° tem Sigmoid)
            predicted = (outputs > 0.5).float()
            total_train += labels.size(0)
            correct_train += (predicted == labels).sum().item()

        epoch_loss = running_loss / len(train_dataset)
        epoch_acc = correct_train / total_train
        history['loss'].append(epoch_loss)
        history['acc'].append(epoch_acc)

        # --------------------- FASE DE VALIDA√á√ÉO ---------------------
        model.eval()
        val_running_loss = 0.0
        correct_val = 0
        total_val = 0
        with torch.no_grad(): # Desativa o c√°lculo de gradientes
            for images, labels in val_loader:
                images, labels = images.to(device), labels.to(device)
                labels = labels.float().unsqueeze(1)

                outputs = model(images)
                loss = criterion(outputs, labels)

                val_running_loss += loss.item() * images.size(0)
                predicted = (outputs > 0.5).float()
                total_val += labels.size(0)
                correct_val += (predicted == labels).sum().item()

        val_epoch_loss = val_running_loss / len(validation_dataset)
        val_epoch_acc = correct_val / total_val
        history['val_loss'].append(val_epoch_loss)
        history['val_acc'].append(val_epoch_acc)

        print(f"Epoch {epoch+1:2d}/{start_epoch + num_epochs}, "
              f"Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}, "
              f"Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}")

# ==============================================================================
#                               EXECU√á√ÉO: TREINAMENTO INICIAL (QUEST√ïES 3 & 4)
# ==============================================================================

# Hiperpar√¢metros
BATCH_SIZE = 20
NUM_EPOCHS = 10
LR = 0.002
MOMENTUM = 0.8

# 1. Defini√ß√£o das Transforma√ß√µes Iniciais (Sem Augmentation)
train_transforms_initial = transforms.Compose([
    transforms.Resize((200, 200)),
    transforms.ToTensor(),
    # Normaliza√ß√£o ImageNet (Padr√£o para modelos pr√©-treinados, aqui usado por conven√ß√£o)
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])

# As transforma√ß√µes de teste/valida√ß√£o n√£o incluem augmentations
test_transforms = train_transforms_initial 

# 2. Carregamento dos Datasets (Inicial)
train_dataset = datasets.ImageFolder(root='./train', transform=train_transforms_initial)
test_dataset = datasets.ImageFolder(root='./test', transform=test_transforms)

# 3. DataLoaders (Shuffle=True para treino, False para teste)
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

# 4. Instancia√ß√£o do Modelo, Otimizador e Fun√ß√£o de Perda
model = HairTypeClassifier().to(device)
optimizer = optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM)

# Usamos nn.BCELoss() porque o modelo j√° aplica Sigmoid na sa√≠da
criterion = nn.BCELoss() 
# Se tiv√©ssemos removido o Sigmoid do modelo, usar√≠amos nn.BCEWithLogitsLoss()

history = {'acc': [], 'loss': [], 'val_acc': [], 'val_loss': []}

# 5. Executa Treinamento Inicial (Epochs 1-10)
train_and_validate(
    model, criterion, optimizer, train_loader, test_loader, 
    num_epochs=NUM_EPOCHS, start_epoch=0, history=history, 
    train_dataset=train_dataset, validation_dataset=test_dataset, 
    use_sigmoid_in_model=True
)

# ==============================================================================
#                 EXECU√á√ÉO: TREINAMENTO COM AUGMENTATION (QUEST√ïES 5 & 6)
# ==============================================================================

print("\n" + "="*80)
print("INICIANDO FASE 2: TREINAMENTO COM DATA AUGMENTATION (Epochs 11-20)")
print("="*80)

# 1. Defini√ß√£o das Transforma√ß√µes com Data Augmentation
train_transforms_aug = transforms.Compose([
    transforms.Resize((200, 200)),
    # Augmentations adicionadas
    transforms.RandomRotation(50), 
    transforms.RandomResizedCrop(200, scale=(0.9, 1.0), ratio=(0.9, 1.1)),
    transforms.RandomHorizontalFlip(),
    # Convers√£o e Normaliza√ß√£o (devem ser as √∫ltimas)
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])

# 2. Re-carregamento do Dataset de Treino com Novas Transforma√ß√µes
train_dataset_aug = datasets.ImageFolder(root='./train', transform=train_transforms_aug)

# 3. Novo DataLoader de Treino
train_loader_aug = DataLoader(train_dataset_aug, batch_size=BATCH_SIZE, shuffle=True)

# 4. Continua o treinamento do *mesmo modelo* por mais 10 √©pocas
train_and_validate(
    model, criterion, optimizer, train_loader_aug, test_loader, 
    num_epochs=NUM_EPOCHS, start_epoch=NUM_EPOCHS, history=history, 
    train_dataset=train_dataset_aug, validation_dataset=test_dataset, 
    use_sigmoid_in_model=True
)

# ==============================================================================
#                            RESUMO DOS RESULTADOS
# ==============================================================================

# Coleta os resultados do hist√≥rico para responder √†s quest√µes
train_acc_q3 = history['acc'][:NUM_EPOCHS] # Acur√°cia das √âpocas 1-10
train_loss_q4 = history['loss'][:NUM_EPOCHS] # Perda das √âpocas 1-10
val_loss_q5 = history['val_loss'][NUM_EPOCHS:] # Perda de Valida√ß√£o das √âpocas 11-20
val_acc_q6 = history['val_acc'][-5:] # Acur√°cia de Valida√ß√£o das √∫ltimas 5 √©pocas (16-20)

print("\n" + "#"*80)
print("# RESUMO DAS M√âTRICAS PARA AS QUEST√ïES:")
print("#"*80)

# Resposta Quest√£o 3: Mediana da Acur√°cia de Treino (Epochs 1-10)
median_acc_q3 = np.median(train_acc_q3)
print(f"Q3: Mediana da Acur√°cia de Treino (Epochs 1-10): {median_acc_q3:.4f}")

# Resposta Quest√£o 4: Desvio Padr√£o da Perda de Treino (Epochs 1-10)
std_loss_q4 = np.std(train_loss_q4)
print(f"Q4: Desvio Padr√£o da Perda de Treino (Epochs 1-10): {std_loss_q4:.4f}")

# Resposta Quest√£o 5: M√©dia da Perda de Teste (Epochs 11-20)
mean_val_loss_q5 = np.mean(val_loss_q5)
print(f"Q5: M√©dia da Perda de Teste (Valida√ß√£o) com Augmentation (Epochs 11-20): {mean_val_loss_q5:.4f}")

# Resposta Quest√£o 6: M√©dia da Acur√°cia de Teste (Epochs 16-20)
mean_val_acc_q6 = np.mean(val_acc_q6)
print(f"Q6: M√©dia da Acur√°cia de Teste (Valida√ß√£o) (√öltimas 5 Augmentation): {mean_val_acc_q6:.4f}")
print("#"*80)
